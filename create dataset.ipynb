{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71da4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da2e8d",
   "metadata": {},
   "source": [
    "## load tokenizer if already saved. else ignore the next 3 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240af47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        tokenizer = data['tokenizer']\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1534c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_lyr = load_tokenizer(\"tokenizers/tokenizer_lyr.pkl\")\n",
    "tokenizer_note = load_tokenizer(\"tokenizers/tokenizer_note.pkl\")\n",
    "tokenizer_duration = load_tokenizer(\"tokenizers/tokenizer_duration.pkl\")\n",
    "tokenizer_rest = load_tokenizer(\"tokenizers/tokenizer_rest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad2481e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 22, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_size = len(tokenizer_note.word_index) + 1\n",
    "durations_size = len(tokenizer_duration.word_index) + 1\n",
    "rests_size = len(tokenizer_rest.word_index) + 1\n",
    "\n",
    "notes_size, durations_size, rests_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db2aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ff18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_path = './lmd-full_MIDI_dataset/Syllable_Parsing'\n",
    "songs = os.listdir(songs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a82e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41800c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7597/7597 [00:08<00:00, 910.19it/s]\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for song in tqdm(songs):\n",
    "    features.append(np.load(f\"{songs_path}/{song}\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69470843",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = np.array(features)[:, :, 2]\n",
    "midis = np.array(features)[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "889ff503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7597/7597 [00:00<00:00, 158795.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7597/7597 [00:04<00:00, 1659.38it/s]\n"
     ]
    }
   ],
   "source": [
    "lyrics_list = []\n",
    "for lyric in tqdm(lyrics):\n",
    "    l = \"BOS \" + \" \".join(lyric[0][:max_len]) + \" EOS\"\n",
    "    lyrics_list.append(l)\n",
    "\n",
    "notes_list, durations_list, rests_list = [], [], []\n",
    "for midi in tqdm(midis):\n",
    "    note = np.array(midi[0])[:, 0]\n",
    "    duration = np.array(midi[0])[:, 1]\n",
    "    rest = np.array(midi[0])[:, 2]\n",
    "    \n",
    "    note_str = \"BOS \" + \" \".join([str(n) for n in note[:max_len]]) + \" EOS\"\n",
    "    duration_str = \"BOS \" + \" \".join([str(d) for d in duration[:max_len]]) + \" EOS\"\n",
    "    rest_str = \"BOS \" + \" \".join([str(r) for r in rest[:max_len]]) + \" EOS\"\n",
    "    \n",
    "    notes_list.append(note_str)\n",
    "    durations_list.append(duration_str)\n",
    "    rests_list.append(rest_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lyric sequences\n",
    "tokenizer_lyr = Tokenizer(num_words=10000, oov_token=\"oov\")\n",
    "tokenizer_lyr.fit_on_texts(lyrics_list)\n",
    "\n",
    "pad_id_lyr = tokenizer_lyr.word_index[\"eos\"]\n",
    "start_id_lyr = tokenizer_lyr.word_index[\"bos\"]\n",
    "\n",
    "sequences_lyr = tokenizer_lyr.texts_to_sequences(lyrics_list)\n",
    "sequences_lyr = pad_sequences(sequences_lyr, maxlen=max_len+2, truncating='post', padding='post', value=pad_id_lyr)\n",
    "\n",
    "# create note sequences\n",
    "tokenizer_note = Tokenizer(filters=\"\")\n",
    "tokenizer_note.fit_on_texts(notes_list)\n",
    "\n",
    "pad_id_note = tokenizer_note.word_index[\"eos\"]\n",
    "start_id_note = tokenizer_note.word_index[\"bos\"]\n",
    "\n",
    "sequences_note = tokenizer_note.texts_to_sequences(notes_list)\n",
    "sequences_note = pad_sequences(sequences_note, truncating='post', padding='post', value=pad_id_note)\n",
    "\n",
    "# create duration sequences\n",
    "tokenizer_duration = Tokenizer(filters=\"\")\n",
    "tokenizer_duration.fit_on_texts(durations_list)\n",
    "\n",
    "pad_id_duration = tokenizer_duration.word_index[\"eos\"]\n",
    "start_id_duration = tokenizer_duration.word_index[\"bos\"]\n",
    "\n",
    "sequences_duration = tokenizer_duration.texts_to_sequences(durations_list)\n",
    "sequences_duration = pad_sequences(sequences_duration, truncating='post', padding='post', value=pad_id_duration)\n",
    "\n",
    "# create rest sequences\n",
    "tokenizer_rest = Tokenizer(filters=\"\")\n",
    "tokenizer_rest.fit_on_texts(rests_list)\n",
    "\n",
    "pad_id_rest = tokenizer_rest.word_index[\"eos\"]\n",
    "start_id_rest = tokenizer_rest.word_index[\"bos\"]\n",
    "\n",
    "sequences_rests = tokenizer_rest.texts_to_sequences(rests_list)\n",
    "sequences_rests = pad_sequences(sequences_rests, truncating='post', padding='post', value=pad_id_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357116e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_lyr = np.expand_dims(sequences_lyr, axis=2)\n",
    "sequences_note = np.expand_dims(sequences_note, axis=2)\n",
    "sequences_duration = np.expand_dims(sequences_duration, axis=2)\n",
    "sequences_rests = np.expand_dims(sequences_rests, axis=2)\n",
    "\n",
    "sequences = np.concatenate([sequences_lyr, sequences_note, sequences_duration, sequences_rests], axis=2)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b89a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"tokenizers\"):\n",
    "    os.mkdir(\"tokenizers\")\n",
    "    \n",
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc836e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/sequences.npy\", sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(file, tokenizer):\n",
    "    with open(file, 'wb') as handle:\n",
    "        pickle.dump({'tokenizer': tokenizer}, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4738414",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tokenizer(\"tokenizers/tokenizer_lyr.pkl\", tokenizer_lyr)\n",
    "save_tokenizer(\"tokenizers/tokenizer_note.pkl\", tokenizer_note)\n",
    "save_tokenizer(\"tokenizers/tokenizer_duration.pkl\", tokenizer_duration)\n",
    "save_tokenizer(\"tokenizers/tokenizer_rest.pkl\", tokenizer_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394a5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
