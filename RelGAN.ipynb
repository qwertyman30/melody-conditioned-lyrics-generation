{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb8c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "from numba import cuda\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from nltk.metrics.scores import (precision, recall)\n",
    "\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4e465",
   "metadata": {},
   "source": [
    "## Preprocessing, load tokenizers, set seed, initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seed = 9248\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.load(\"data/sequences.npy\")\n",
    "print(data_matrix.shape)\n",
    "\n",
    "lyrics = data_matrix[:, :, 0]\n",
    "notes = data_matrix[:, :, 1]\n",
    "durations = data_matrix[:, :, 2]\n",
    "rests = data_matrix[:, :, 3]\n",
    "\n",
    "print(lyrics.shape, notes.shape, durations.shape, rests.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba6a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        tokenizer = data['tokenizer']\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6227bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_lyr = load_tokenizer(\"tokenizers/tokenizer_lyr.pkl\")\n",
    "tokenizer_note = load_tokenizer(\"tokenizers/tokenizer_note.pkl\")\n",
    "tokenizer_duration = load_tokenizer(\"tokenizers/tokenizer_duration.pkl\")\n",
    "tokenizer_rest = load_tokenizer(\"tokenizers/tokenizer_rest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e955ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_songs = len(lyrics)\n",
    "num_samples = 6848\n",
    "train_inds = np.random.choice(np.arange(num_songs), size=num_samples, replace=False)\n",
    "test_inds = np.delete(np.arange(num_songs), train_inds)\n",
    "\n",
    "train_lyrics = [lyrics[i] for i in train_inds]\n",
    "test_lyrics = [lyrics[i] for i in test_inds]\n",
    "\n",
    "train_notes = [notes[i] for i in train_inds]\n",
    "test_notes = [notes[i] for i in test_inds]\n",
    "\n",
    "train_durations = [durations[i] for i in train_inds]\n",
    "test_durations = [durations[i] for i in test_inds]\n",
    "\n",
    "train_rests = [rests[i] for i in train_inds]\n",
    "test_rests = [rests[i] for i in test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc78037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General params\n",
    "vocab_size = min(len(tokenizer_lyr.word_index) + 1, 10000)\n",
    "notes_size = len(tokenizer_note.word_index) + 1\n",
    "durations_size = len(tokenizer_duration.word_index) + 1\n",
    "rests_size = len(tokenizer_rest.word_index) + 1\n",
    "\n",
    "pad_id_lyr = tokenizer_lyr.word_index[\"eos\"]\n",
    "start_id_lyr = tokenizer_lyr.word_index[\"bos\"]\n",
    "\n",
    "pad_id_note = tokenizer_note.word_index[\"eos\"]\n",
    "start_id_note = tokenizer_note.word_index[\"bos\"]\n",
    "\n",
    "pad_id_duration = tokenizer_duration.word_index[\"eos\"]\n",
    "start_id_duration = tokenizer_duration.word_index[\"bos\"]\n",
    "\n",
    "pad_id_rest = tokenizer_rest.word_index[\"eos\"]\n",
    "start_id_rest = tokenizer_rest.word_index[\"bos\"]\n",
    "\n",
    "max_seq_len = lyrics.shape[1]\n",
    "batch_size = 16\n",
    "\n",
    "chencherry = SmoothingFunction()\n",
    "\n",
    "# Generator params\n",
    "g_dropout = 0.3\n",
    "g_embed_dim = 32\n",
    "g_hidden = 32\n",
    "pretrain_epochs_gen = 120\n",
    "g_lr_pretrain = 0.01\n",
    "g_lr_adv = 1e-4\n",
    "\n",
    "# Discriminator params\n",
    "d_embed_dim = 64\n",
    "dis_filter_sizes = [2, 3, 4, 5]\n",
    "dis_num_filters = [300, 300, 300, 300]\n",
    "d_dropout_prob = 0.2\n",
    "d_lr_pre = 1e-3\n",
    "d_lr_adv = 1e-4\n",
    "num_rep = 64\n",
    "\n",
    "# relational memory params\n",
    "mem_slots = 1\n",
    "num_heads = 2\n",
    "head_size = 256\n",
    "\n",
    "clip_norm = 5.0\n",
    "\n",
    "adversarial_epochs = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acbe2f9",
   "metadata": {},
   "source": [
    "## Initialize Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f50d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal_(tensor, mean=0, std=1):\n",
    "    \"\"\"\n",
    "    Implemented by @ruotianluo\n",
    "    See https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/15\n",
    "    \"\"\"\n",
    "    size = tensor.shape\n",
    "    tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "    valid = (tmp < 2) & (tmp > -2)\n",
    "    ind = valid.max(-1, keepdim=True)[1]\n",
    "    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "    tensor.data.mul_(std).add_(mean)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c780deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Constructs a `RelationalMemory` object.\n",
    "    This class is same as the RMC from relational_rnn_models.py, but without language modeling-specific variables.\n",
    "    Args:\n",
    "      mem_slots: The total number of memory slots to use.\n",
    "      head_size: The size of an attention head.\n",
    "      input_size: The size of input per step. i.e. the dimension of each input vector\n",
    "      num_heads: The number of attention heads to use. Defaults to 1.\n",
    "      num_blocks: Number of times to compute attention per time step. Defaults\n",
    "        to 1.\n",
    "      forget_bias: Bias to use for the forget gate, assuming we are using\n",
    "        some form of gating. Defaults to 1.\n",
    "      input_bias: Bias to use for the input gate, assuming we are using\n",
    "        some form of gating. Defaults to 0.\n",
    "      gate_style: Whether to use per-element gating ('unit'),\n",
    "        per-memory slot gating ('memory'), or no gating at all (None).\n",
    "        Defaults to `unit`.\n",
    "      attention_mlp_layers: Number of layers to use in the post-attention\n",
    "        MLP. Defaults to 2.\n",
    "      key_size: Size of vector to use for key & query vectors in the attention\n",
    "        computation. Defaults to None, in which case we use `head_size`.\n",
    "      # NEW flag for this class\n",
    "      return_all_outputs: Whether the model returns outputs for each step (like seq2seq) or only the final output.\n",
    "    Raises:\n",
    "      ValueError: gate_style not one of [None, 'memory', 'unit'].\n",
    "      ValueError: num_blocks is < 1.\n",
    "      ValueError: attention_mlp_layers is < 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mem_slots, head_size, input_size, num_heads=1, num_blocks=1, forget_bias=1., input_bias=0.,\n",
    "                 gate_style='unit', attention_mlp_layers=2, key_size=None, return_all_outputs=False):\n",
    "        super(RelationalMemory, self).__init__()\n",
    "\n",
    "        ########## generic parameters for RMC ##########\n",
    "        self.mem_slots = mem_slots\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.mem_size = self.head_size * self.num_heads\n",
    "\n",
    "        # a new fixed params needed for pytorch port of RMC\n",
    "        # +1 is the concatenated input per time step : we do self-attention with the concatenated memory & input\n",
    "        # so if the mem_slots = 1, this value is 2\n",
    "        self.mem_slots_plus_input = self.mem_slots + 1\n",
    "\n",
    "        if num_blocks < 1:\n",
    "            raise ValueError('num_blocks must be >=1. Got: {}.'.format(num_blocks))\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        if gate_style not in ['unit', 'memory', None]:\n",
    "            raise ValueError(\n",
    "                'gate_style must be one of [\\'unit\\', \\'memory\\', None]. got: '\n",
    "                '{}.'.format(gate_style))\n",
    "        self.gate_style = gate_style\n",
    "\n",
    "        if attention_mlp_layers < 1:\n",
    "            raise ValueError('attention_mlp_layers must be >= 1. Got: {}.'.format(\n",
    "                attention_mlp_layers))\n",
    "        self.attention_mlp_layers = attention_mlp_layers\n",
    "\n",
    "        self.key_size = key_size if key_size else self.head_size\n",
    "\n",
    "        ########## parameters for multihead attention ##########\n",
    "        # value_size is same as head_size\n",
    "        self.value_size = self.head_size\n",
    "        # total size for query-key-value\n",
    "        self.qkv_size = 2 * self.key_size + self.value_size\n",
    "        self.total_qkv_size = self.qkv_size * self.num_heads  # denoted as F\n",
    "\n",
    "        # each head has qkv_sized linear projector\n",
    "        # just using one big param is more efficient, rather than this line\n",
    "        # self.qkv_projector = [nn.Parameter(torch.randn((self.qkv_size, self.qkv_size))) for _ in range(self.num_heads)]\n",
    "        self.qkv_projector = nn.Linear(self.mem_size, self.total_qkv_size)\n",
    "        self.qkv_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.total_qkv_size])\n",
    "\n",
    "        # used for attend_over_memory function\n",
    "        self.attention_mlp = nn.ModuleList([nn.Linear(self.mem_size, self.mem_size)] * self.attention_mlp_layers)\n",
    "        self.attended_memory_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n",
    "        self.attended_memory_layernorm2 = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n",
    "\n",
    "        ########## parameters for initial embedded input projection ##########\n",
    "        self.input_size = input_size\n",
    "        self.input_projector = nn.Linear(self.input_size, self.mem_size)\n",
    "\n",
    "        ########## parameters for gating ##########\n",
    "        self.num_gates = 2 * self.calculate_gate_size()\n",
    "        self.input_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n",
    "        self.memory_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n",
    "        # trainable scalar gate bias tensors\n",
    "        self.forget_bias = nn.Parameter(torch.tensor(forget_bias, dtype=torch.float32))\n",
    "        self.input_bias = nn.Parameter(torch.tensor(input_bias, dtype=torch.float32))\n",
    "\n",
    "        ########## number of outputs returned #####\n",
    "        self.return_all_outputs = return_all_outputs\n",
    "\n",
    "    def repackage_hidden(self, h):\n",
    "        \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "        # needed for truncated BPTT, called at every batch forward pass\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "\n",
    "    def initial_state(self, batch_size, trainable=False):\n",
    "        \"\"\"\n",
    "        Creates the initial memory.\n",
    "        We should ensure each row of the memory is initialized to be unique,\n",
    "        so initialize the matrix to be the identity. We then pad or truncate\n",
    "        as necessary so that init_state is of size\n",
    "        (batch_size, self.mem_slots, self.mem_size).\n",
    "        Args:\n",
    "          batch_size: The size of the batch.\n",
    "          trainable: Whether the initial state is trainable. This is always True.\n",
    "        Returns:\n",
    "          init_state: A truncated or padded matrix of size\n",
    "            (batch_size, self.mem_slots, self.mem_size).\n",
    "        \"\"\"\n",
    "        init_state = torch.stack([torch.eye(self.mem_slots) for _ in range(batch_size)])\n",
    "\n",
    "        # pad the matrix with zeros\n",
    "        if self.mem_size > self.mem_slots:\n",
    "            difference = self.mem_size - self.mem_slots\n",
    "            pad = torch.zeros((batch_size, self.mem_slots, difference))\n",
    "            init_state = torch.cat([init_state, pad], -1)\n",
    "\n",
    "        # truncation. take the first 'self.mem_size' components\n",
    "        elif self.mem_size < self.mem_slots:\n",
    "            init_state = init_state[:, :, :self.mem_size]\n",
    "\n",
    "        return init_state\n",
    "\n",
    "    def multihead_attention(self, memory):\n",
    "        \"\"\"\n",
    "        Perform multi-head attention from 'Attention is All You Need'.\n",
    "        Implementation of the attention mechanism from\n",
    "        https://arxiv.org/abs/1706.03762.\n",
    "        Args:\n",
    "          memory: Memory tensor to perform attention on.\n",
    "        Returns:\n",
    "          new_memory: New memory tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # First, a simple linear projection is used to construct queries\n",
    "        qkv = self.qkv_projector(memory)\n",
    "        # apply layernorm for every dim except the batch dim\n",
    "        qkv = self.qkv_layernorm(qkv)\n",
    "\n",
    "        # mem_slots needs to be dynamically computed since mem_slots got concatenated with inputs\n",
    "        # example: self.mem_slots=10 and seq_length is 3, and then mem_slots is 10 + 1 = 11 for each 3 step forward pass\n",
    "        # this is the same as self.mem_slots_plus_input, but defined to keep the sonnet implementation code style\n",
    "        mem_slots = memory.shape[1]  # denoted as N\n",
    "\n",
    "        # split the qkv to multiple heads H\n",
    "        # [B, N, F] => [B, N, H, F/H]\n",
    "        qkv_reshape = qkv.view(qkv.shape[0], mem_slots, self.num_heads, self.qkv_size)\n",
    "\n",
    "        # [B, N, H, F/H] => [B, H, N, F/H]\n",
    "        qkv_transpose = qkv_reshape.permute(0, 2, 1, 3)\n",
    "\n",
    "        # [B, H, N, key_size], [B, H, N, key_size], [B, H, N, value_size]\n",
    "        q, k, v = torch.split(qkv_transpose, [self.key_size, self.key_size, self.value_size], -1)\n",
    "\n",
    "        # scale q with d_k, the dimensionality of the key vectors\n",
    "        q *= (self.key_size ** -0.5)\n",
    "\n",
    "        # make it [B, H, N, N]\n",
    "        dot_product = torch.matmul(q, k.permute(0, 1, 3, 2))\n",
    "        weights = F.softmax(dot_product, dim=-1)\n",
    "\n",
    "        # output is [B, H, N, V]\n",
    "        output = torch.matmul(weights, v)\n",
    "\n",
    "        # [B, H, N, V] => [B, N, H, V] => [B, N, H*V]\n",
    "        output_transpose = output.permute(0, 2, 1, 3).contiguous()\n",
    "        new_memory = output_transpose.view((output_transpose.shape[0], output_transpose.shape[1], -1))\n",
    "\n",
    "        return new_memory\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return [self.mem_slots, self.mem_size]\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.mem_slots * self.mem_size\n",
    "\n",
    "    def calculate_gate_size(self):\n",
    "        \"\"\"\n",
    "        Calculate the gate size from the gate_style.\n",
    "        Returns:\n",
    "          The per sample, per head parameter size of each gate.\n",
    "        \"\"\"\n",
    "        if self.gate_style == 'unit':\n",
    "            return self.mem_size\n",
    "        elif self.gate_style == 'memory':\n",
    "            return 1\n",
    "        else:  # self.gate_style == None\n",
    "            return 0\n",
    "\n",
    "    def create_gates(self, inputs, memory):\n",
    "        \"\"\"\n",
    "        Create input and forget gates for this step using `inputs` and `memory`.\n",
    "        Args:\n",
    "          inputs: Tensor input.\n",
    "          memory: The current state of memory.\n",
    "        Returns:\n",
    "          input_gate: A LSTM-like insert gate.\n",
    "          forget_gate: A LSTM-like forget gate.\n",
    "        \"\"\"\n",
    "        # We'll create the input and forget gates at once. Hence, calculate double\n",
    "        # the gate size.\n",
    "\n",
    "        # equation 8: since there is no output gate, h is just a tanh'ed m\n",
    "        memory = torch.tanh(memory)\n",
    "\n",
    "        # sonnet uses this, but i think it assumes time step of 1 for all cases\n",
    "        # if inputs is (B, T, features) where T > 1, this gets incorrect\n",
    "        # inputs = inputs.view(inputs.shape[0], -1)\n",
    "\n",
    "        # fixed implementation\n",
    "        if len(inputs.shape) == 3:\n",
    "            if inputs.shape[1] > 1:\n",
    "                raise ValueError(\n",
    "                    \"input seq length is larger than 1. create_gate function is meant to be called for each step, with input seq length of 1\")\n",
    "            inputs = inputs.view(inputs.shape[0], -1)\n",
    "            # matmul for equation 4 and 5\n",
    "            # there is no output gate, so equation 6 is not implemented\n",
    "            gate_inputs = self.input_gate_projector(inputs)\n",
    "            gate_inputs = gate_inputs.unsqueeze(dim=1)\n",
    "            gate_memory = self.memory_gate_projector(memory)\n",
    "        else:\n",
    "            raise ValueError(\"input shape of create_gate function is 2, expects 3\")\n",
    "\n",
    "        # this completes the equation 4 and 5\n",
    "        gates = gate_memory + gate_inputs\n",
    "        gates = torch.split(gates, split_size_or_sections=int(gates.shape[2] / 2), dim=2)\n",
    "        input_gate, forget_gate = gates\n",
    "        assert input_gate.shape[2] == forget_gate.shape[2]\n",
    "\n",
    "        # to be used for equation 7\n",
    "        input_gate = torch.sigmoid(input_gate + self.input_bias)\n",
    "        forget_gate = torch.sigmoid(forget_gate + self.forget_bias)\n",
    "\n",
    "        return input_gate, forget_gate\n",
    "\n",
    "    def attend_over_memory(self, memory):\n",
    "        \"\"\"\n",
    "        Perform multiheaded attention over `memory`.\n",
    "            Args:\n",
    "              memory: Current relational memory.\n",
    "            Returns:\n",
    "              The attended-over memory.\n",
    "        \"\"\"\n",
    "        for _ in range(self.num_blocks):\n",
    "            attended_memory = self.multihead_attention(memory)\n",
    "\n",
    "            # Add a skip connection to the multiheaded attention's input.\n",
    "            memory = self.attended_memory_layernorm(memory + attended_memory)\n",
    "\n",
    "            # add a skip connection to the attention_mlp's input.\n",
    "            attention_mlp = memory\n",
    "            for i, l in enumerate(self.attention_mlp):\n",
    "                attention_mlp = self.attention_mlp[i](attention_mlp)\n",
    "                attention_mlp = F.relu(attention_mlp)\n",
    "            memory = self.attended_memory_layernorm2(memory + attention_mlp)\n",
    "\n",
    "        return memory\n",
    "\n",
    "    def forward_step(self, inputs, memory, treat_input_as_matrix=False):\n",
    "        \"\"\"\n",
    "        Forward step of the relational memory core.\n",
    "        Args:\n",
    "          inputs: Tensor input.\n",
    "          memory: Memory output from the previous time step.\n",
    "          treat_input_as_matrix: Optional, whether to treat `input` as a sequence\n",
    "            of matrices. Default to False, in which case the input is flattened\n",
    "            into a vector.\n",
    "        Returns:\n",
    "          output: This time step's output.\n",
    "          next_memory: The next version of memory to use.\n",
    "        \"\"\"\n",
    "\n",
    "        if treat_input_as_matrix:\n",
    "            # keep (Batch, Seq, ...) dim (0, 1), flatten starting from dim 2\n",
    "            inputs = inputs.view(inputs.shape[0], inputs.shape[1], -1)\n",
    "            # apply linear layer for dim 2\n",
    "            inputs_reshape = self.input_projector(inputs)\n",
    "        else:\n",
    "            # keep (Batch, ...) dim (0), flatten starting from dim 1\n",
    "            inputs = inputs.view(inputs.shape[0], -1)\n",
    "            # apply linear layer for dim 1\n",
    "            inputs = self.input_projector(inputs)\n",
    "            # unsqueeze the time step to dim 1\n",
    "            inputs_reshape = inputs.unsqueeze(dim=1)\n",
    "\n",
    "        memory_plus_input = torch.cat([memory, inputs_reshape], dim=1)\n",
    "        next_memory = self.attend_over_memory(memory_plus_input)\n",
    "\n",
    "        # cut out the concatenated input vectors from the original memory slots\n",
    "        n = inputs_reshape.shape[1]\n",
    "        next_memory = next_memory[:, :-n, :]\n",
    "\n",
    "        if self.gate_style == 'unit' or self.gate_style == 'memory':\n",
    "            # these gates are sigmoid-applied ones for equation 7\n",
    "            input_gate, forget_gate = self.create_gates(inputs_reshape, memory)\n",
    "            # equation 7 calculation\n",
    "            next_memory = input_gate * torch.tanh(next_memory)\n",
    "            next_memory += forget_gate * memory\n",
    "\n",
    "        output = next_memory.view(next_memory.shape[0], -1)\n",
    "\n",
    "        return output, next_memory\n",
    "\n",
    "    def forward(self, inputs, memory, treat_input_as_matrix=False):\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "\n",
    "        # for loop implementation of (entire) recurrent forward pass of the model\n",
    "        # inputs is batch first [batch, seq], and output logit per step is [batch, vocab]\n",
    "        # so the concatenated logits are [seq * batch, vocab]\n",
    "\n",
    "        # targets are flattened [seq, batch] => [seq * batch], so the dimension is correct\n",
    "\n",
    "        # memory = self.repackage_hidden(memory)\n",
    "        logit = 0\n",
    "        logits = []\n",
    "        # shape[1] is seq_lenth T\n",
    "        for idx_step in range(inputs.shape[1]):\n",
    "            logit, memory = self.forward_step(inputs[:, idx_step], memory)\n",
    "            logits.append(logit.unsqueeze(1))\n",
    "        logits = torch.cat(logits, dim=1)\n",
    "\n",
    "        if self.return_all_outputs:\n",
    "            return logits, memory\n",
    "        else:\n",
    "            return logit.unsqueeze(1), memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f9d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, notes_size, durations_size, rests_size,\n",
    "                 max_seq_len, pad_id_lyr, pad_id_note, pad_id_duration, pad_id_rest, gpu=True):\n",
    "        super(LSTMGenerator, self).__init__()\n",
    "        self.name = 'vanilla'\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_id_lyr = pad_id_lyr\n",
    "        self.pad_id_note = pad_id_note\n",
    "        self.pad_id_duration = pad_id_duration\n",
    "        self.pad_id_rest = pad_id_rest\n",
    "        self.gpu = gpu\n",
    "\n",
    "        self.temperature = 1.0\n",
    "\n",
    "        self.embeddings_lyr = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_id_lyr)\n",
    "        self.embeddings_notes = nn.Embedding(notes_size, embedding_dim, padding_idx=pad_id_note)\n",
    "        self.embeddings_durations = nn.Embedding(durations_size, embedding_dim, padding_idx=pad_id_duration)\n",
    "        self.embeddings_rests = nn.Embedding(rests_size, embedding_dim, padding_idx=pad_id_rest)\n",
    "        self.lstm = nn.LSTM(embedding_dim*4, hidden_dim, batch_first=True)\n",
    "        self.lstm2out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, inp, mel, hidden, need_hidden=False):\n",
    "        \"\"\"\n",
    "        Embeds input and applies LSTM\n",
    "        :param inp: batch_size * seq_len\n",
    "        :param hidden: (h, c)\n",
    "        :param need_hidden: if return hidden, use for sampling\n",
    "        \"\"\"\n",
    "        if len(mel.shape) == 2:\n",
    "            notes = mel[:, 0]\n",
    "            durations = mel[:, 1]\n",
    "            rests = mel[:, 2]\n",
    "        else:\n",
    "            notes = mel[:, :, 0]\n",
    "            durations = mel[:, :, 1]\n",
    "            rests = mel[:, :, 2]\n",
    "\n",
    "        emb_lyr = self.embeddings_lyr(inp)\n",
    "        emb_note = self.embeddings_notes(notes)\n",
    "        emb_dur = self.embeddings_durations(durations)\n",
    "        emb_rest = self.embeddings_rests(rests)\n",
    "\n",
    "        emb = torch.cat([emb_lyr, emb_note, emb_dur, emb_rest], dim=2)\n",
    "\n",
    "        if len(inp.size()) == 1:\n",
    "            emb = emb.unsqueeze(1)  # batch_size * 1 * embedding_dim\n",
    "\n",
    "        out, hidden = self.lstm(emb, hidden)  # out: batch_size * seq_len * hidden_dim\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)  # out: (batch_size * len) * hidden_dim\n",
    "        out = self.lstm2out(out)  # (batch_size * seq_len) * vocab_size\n",
    "        # out = self.temperature * out  # temperature\n",
    "        pred = self.softmax(out)\n",
    "\n",
    "        if need_hidden:\n",
    "            return pred, hidden\n",
    "        else:\n",
    "            return pred\n",
    "\n",
    "    def sample(self, melody, num_samples, batch_size, start_letter):\n",
    "        \"\"\"\n",
    "        Samples the network and returns num_samples samples of length max_seq_len.\n",
    "        :return samples: num_samples * max_seq_length (a sampled sequence in each row)\n",
    "        \"\"\"\n",
    "        num_batch = num_samples // batch_size + 1 if num_samples != batch_size else 1\n",
    "        samples = torch.zeros(num_batch * batch_size, self.max_seq_len).long()\n",
    "\n",
    "        # Generate sentences with multinomial sampling strategy\n",
    "        for b in range(num_batch):\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "            inp = torch.LongTensor([start_letter] * batch_size)\n",
    "            if self.gpu:\n",
    "                inp = inp.cuda()\n",
    "\n",
    "            for i in range(self.max_seq_len):\n",
    "                y = melody[:, i]\n",
    "                if self.gpu:\n",
    "                    y = y.cuda()\n",
    "                out, hidden = self.forward(inp, y, hidden, need_hidden=True)  # out: batch_size * vocab_size\n",
    "                next_token = torch.multinomial(torch.exp(out), 1)  # batch_size * 1 (sampling from each row)\n",
    "                samples[b * batch_size:(b + 1) * batch_size, i] = next_token.view(-1)\n",
    "                inp = next_token.view(-1)\n",
    "        samples = samples[:num_samples]\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                truncated_normal_(param, std=stddev)\n",
    "\n",
    "    def init_hidden(self, batch_size=batch_size):\n",
    "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "\n",
    "        if self.gpu:\n",
    "            return h.cuda(), c.cuda()\n",
    "        else:\n",
    "            return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454679fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelGAN_G(LSTMGenerator):\n",
    "    def __init__(self, mem_slots, num_heads, head_size, embedding_dim, hidden_dim,\n",
    "                 vocab_size, notes_size, durations_size, rests_size, max_seq_len,\n",
    "                 pad_id_lyr, pad_id_note, pad_id_duration, pad_id_rest, gpu=True, model_type=\"RMC\"):\n",
    "        super(RelGAN_G, self).__init__(embedding_dim, hidden_dim, vocab_size, notes_size,\n",
    "                                       durations_size, rests_size, max_seq_len, pad_id_lyr, \n",
    "                                       pad_id_note, pad_id_duration, pad_id_rest, gpu)\n",
    "        self.name = 'relgan'\n",
    "        self.temperature = 1000.0  # max value\n",
    "        self.model_type = model_type\n",
    "        self.embeddings_lyr = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_id_lyr)\n",
    "        self.embeddings_notes = nn.Embedding(notes_size, embedding_dim, padding_idx=pad_id_note)\n",
    "        self.embeddings_durations = nn.Embedding(durations_size, embedding_dim, padding_idx=pad_id_duration)\n",
    "        self.embeddings_rests = nn.Embedding(rests_size, embedding_dim, padding_idx=pad_id_rest)\n",
    "        if model_type == 'LSTM':\n",
    "            # LSTM\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.lstm = nn.LSTM(embedding_dim*4, self.hidden_dim, batch_first=True)\n",
    "            self.lstm2out = nn.Linear(self.hidden_dim, vocab_size)\n",
    "        else:\n",
    "            # RMC\n",
    "            self.hidden_dim = mem_slots * num_heads * head_size\n",
    "            self.lstm = RelationalMemory(mem_slots=mem_slots, head_size=head_size, input_size=embedding_dim*4,\n",
    "                                         num_heads=num_heads, return_all_outputs=True)\n",
    "            self.lstm2out = nn.Linear(self.hidden_dim, vocab_size)\n",
    "\n",
    "        self.init_params()\n",
    "        pass\n",
    "\n",
    "    def init_hidden(self, batch_size=32):\n",
    "        if self.model_type == 'LSTM':\n",
    "            h = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "            c = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "\n",
    "            if self.gpu:\n",
    "                return h.cuda(), c.cuda()\n",
    "            else:\n",
    "                return h, c\n",
    "        else:\n",
    "            \"\"\"init RMC memory\"\"\"\n",
    "            memory = self.lstm.initial_state(batch_size)\n",
    "            memory = self.lstm.repackage_hidden(memory)  # detch memory at first\n",
    "            return memory.cuda() if self.gpu else memory\n",
    "\n",
    "    def step(self, lyr, mel, hidden):\n",
    "        \"\"\"\n",
    "        RelGAN step forward\n",
    "        :param inp: [batch_size]\n",
    "        :param hidden: memory size\n",
    "        :return: pred, hidden, next_token, next_token_onehot, next_o\n",
    "            - pred: batch_size * vocab_size, use for adversarial training backward\n",
    "            - hidden: next hidden\n",
    "            - next_token: [batch_size], next sentence token\n",
    "            - next_token_onehot: batch_size * vocab_size, not used yet\n",
    "            - next_o: batch_size * vocab_size, not used yet\n",
    "        \"\"\"\n",
    "        notes = mel[:, 0]\n",
    "        durations = mel[:, 1]\n",
    "        rests = mel[:, 2]\n",
    "\n",
    "        emb_lyr = self.embeddings_lyr(lyr).unsqueeze(1) # batch_size * len * embedding_dim\n",
    "        emb_note = self.embeddings_notes(notes).unsqueeze(1)\n",
    "        emb_dur = self.embeddings_durations(durations).unsqueeze(1)\n",
    "        emb_rest = self.embeddings_rests(rests).unsqueeze(1)\n",
    "\n",
    "        emb = torch.cat([emb_lyr, emb_note, emb_dur, emb_rest], dim=2)\n",
    "\n",
    "        out, hidden = self.lstm(emb, hidden)\n",
    "        gumbel_t = self.add_gumbel(self.lstm2out(out.squeeze(1)))\n",
    "        next_token = torch.argmax(gumbel_t, dim=1).detach()\n",
    "        # next_token_onehot = F.one_hot(next_token, cfg.vocab_size).float()  # not used yet\n",
    "        next_token_onehot = None\n",
    "\n",
    "        pred = F.softmax(gumbel_t * self.temperature, dim=-1)  # batch_size * vocab_size\n",
    "        # next_o = torch.sum(next_token_onehot * pred, dim=1)  # not used yet\n",
    "        next_o = None\n",
    "\n",
    "        return pred, hidden, next_token, next_token_onehot, next_o\n",
    "\n",
    "    def sample(self, melody, num_samples, batch_size, one_hot=False, start_letter=1):\n",
    "        \"\"\"\n",
    "        Sample from RelGAN Generator\n",
    "        - one_hot: if return pred of RelGAN, used for adversarial training\n",
    "        :return:\n",
    "            - all_preds: batch_size * seq_len * vocab_size, only use for a batch\n",
    "            - samples: all samples\n",
    "        \"\"\"\n",
    "        global all_preds\n",
    "        num_batch = num_samples // batch_size + 1 if num_samples != batch_size else 1\n",
    "        samples = torch.zeros(num_batch * batch_size, self.max_seq_len).long()\n",
    "        if one_hot:\n",
    "            all_preds = torch.zeros(batch_size, self.max_seq_len, self.vocab_size)\n",
    "            if self.gpu:\n",
    "                all_preds = all_preds.cuda()\n",
    "\n",
    "        for b in range(num_batch):\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "            inp = torch.LongTensor([start_letter] * batch_size)\n",
    "            mel_batch = melody[b * batch_size:(b+1) * batch_size, :]\n",
    "            if len(mel_batch) < batch_size:\n",
    "                break\n",
    "            if self.gpu:\n",
    "                inp = inp.cuda()\n",
    "                mel_batch = mel_batch.cuda()\n",
    "\n",
    "            for i in range(self.max_seq_len):\n",
    "                y = mel_batch[:, i]\n",
    "                pred, hidden, next_token, _, _ = self.step(inp, y, hidden)\n",
    "                samples[b * batch_size:(b + 1) * batch_size, i] = next_token\n",
    "                if one_hot:\n",
    "                    all_preds[:, i] = pred\n",
    "                inp = next_token\n",
    "        samples = samples[:num_samples]  # num_samples * seq_len\n",
    "\n",
    "        if one_hot:\n",
    "            return all_preds  # batch_size * seq_len * vocab_size\n",
    "        return samples\n",
    "\n",
    "    @staticmethod\n",
    "    def add_gumbel(o_t, eps=1e-10, gpu=True):\n",
    "        \"\"\"Add o_t by a vector sampled from Gumbel(0,1)\"\"\"\n",
    "        u = torch.zeros(o_t.size())\n",
    "        if gpu:\n",
    "            u = u.cuda()\n",
    "\n",
    "        u.uniform_(0, 1)\n",
    "        g_t = -torch.log(-torch.log(u + eps) + eps)\n",
    "        gumbel_t = o_t + g_t\n",
    "        return gumbel_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDiscriminator(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, filter_sizes, num_filters, padding_idx, gpu=False,\n",
    "                 dropout=0.2):\n",
    "        super(CNNDiscriminator, self).__init__()\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.feature_dim = sum(num_filters)\n",
    "        self.gpu = gpu\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n, (f, embed_dim)) for (n, f) in zip(num_filters, filter_sizes)\n",
    "        ])\n",
    "        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n",
    "        self.feature2out = nn.Linear(self.feature_dim, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Get final predictions of discriminator\n",
    "        :param inp: batch_size * seq_len\n",
    "        :return: pred: batch_size * 2\n",
    "        \"\"\"\n",
    "        feature = self.get_feature(inp)\n",
    "        pred = self.feature2out(self.dropout(feature))\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def get_feature(self, inp):\n",
    "        \"\"\"\n",
    "        Get feature vector of given sentences\n",
    "        :param inp: batch_size * max_seq_len\n",
    "        :return: batch_size * feature_dim\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(inp).unsqueeze(1)  # batch_size * 1 * max_seq_len * embed_dim\n",
    "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]\n",
    "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs]  # [batch_size * num_filter]\n",
    "        pred = torch.cat(pools, 1)  # tensor: batch_size * feature_dim\n",
    "        highway = self.highway(pred)\n",
    "        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred  # highway\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                truncated_normal_(param, std=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b00171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelGAN_D(CNNDiscriminator):\n",
    "    def __init__(self, embed_dim, max_seq_len, num_rep, vocab_size, padding_idx, gpu=True, dropout=0.25):\n",
    "        super(RelGAN_D, self).__init__(embed_dim, vocab_size, dis_filter_sizes, dis_num_filters, padding_idx,\n",
    "                                       gpu, dropout)\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.feature_dim = sum(dis_num_filters)\n",
    "        self.emb_dim_single = int(embed_dim / num_rep)\n",
    "\n",
    "        self.embeddings = nn.Linear(vocab_size, embed_dim, bias=False)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n, (f, self.emb_dim_single), stride=(1, self.emb_dim_single)) for (n, f) in\n",
    "            zip(dis_num_filters, dis_filter_sizes)\n",
    "        ])\n",
    "\n",
    "        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n",
    "        self.feature2out = nn.Linear(self.feature_dim, 100)\n",
    "        self.out2logits = nn.Linear(100, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Get logits of discriminator\n",
    "        :param inp: batch_size * seq_len * vocab_size\n",
    "        :return logits: [batch_size * num_rep] (1-D tensor)\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(inp).unsqueeze(1)  # batch_size * 1 * max_seq_len * embed_dim\n",
    "\n",
    "        cons = [F.relu(conv(emb)) for conv in self.convs]  # [batch_size * num_filter * (seq_len-k_h+1) * num_rep]\n",
    "        pools = [F.max_pool2d(con, (con.size(2), 1)).squeeze(2) for con in cons]  # [batch_size * num_filter * num_rep]\n",
    "        pred = torch.cat(pools, 1)\n",
    "        pred = pred.permute(0, 2, 1).contiguous().view(-1, self.feature_dim)  # (batch_size * num_rep) * feature_dim\n",
    "        highway = self.highway(pred)\n",
    "        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred  # highway\n",
    "\n",
    "        pred = self.feature2out(self.dropout(pred))\n",
    "        logits = self.out2logits(pred).squeeze(1)  # [batch_size * num_rep]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04557efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenDataIter:\n",
    "    def __init__(self, samples, melodies, batch_size, max_seq_len, start_letter=1, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.start_letter = start_letter\n",
    "\n",
    "        self.loader = DataLoader(\n",
    "            dataset=GANDataset(self.__read_data__(samples, melodies)),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=True)\n",
    "\n",
    "        self.input = self._all_data_('lyric')\n",
    "        self.melody = self._all_data_('melody')\n",
    "        self.target = self._all_data_('target')\n",
    "\n",
    "    def __read_data__(self, samples, melodies):\n",
    "        \"\"\"\n",
    "        input: same as target, but start with start_letter.\n",
    "        \"\"\"\n",
    "        # global all_data\n",
    "        inp, target = self.prepare(samples, melodies, self.start_letter)\n",
    "        lyr, mel = inp\n",
    "        all_data = [{'lyric': l, 'melody': m, 'target': t} for (l, m, t) in zip(lyr, mel, target)]\n",
    "        return all_data\n",
    "\n",
    "    def random_batch(self):\n",
    "        \"\"\"Randomly choose a batch from loader, please note that the data should not be shuffled.\"\"\"\n",
    "        idx = random.randint(0, len(self.loader) - 1)\n",
    "        return list(self.loader)[idx]\n",
    "\n",
    "    def _all_data_(self, col):\n",
    "        return torch.cat([data[col].unsqueeze(0) for data in self.loader.dataset.data], 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare(samples, melodies, start_id, gpu=False):\n",
    "        \"\"\"Add start_letter to samples as inp, target same as samples\"\"\"\n",
    "        inp = torch.zeros(samples.size()).long()\n",
    "        target = samples\n",
    "        inp[:, 0] = start_id\n",
    "        inp[:, 1:] = target[:, :max_seq_len - 1]\n",
    "\n",
    "        if gpu:\n",
    "            return (inp.cuda(), melodies.cuda()), target.cuda()\n",
    "        return (inp, melodies), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca358699",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = RelGAN_G(mem_slots, num_heads, head_size, g_embed_dim, g_hidden,\n",
    "               vocab_size, notes_size, durations_size, rests_size, max_seq_len,\n",
    "               pad_id_lyr, pad_id_note, pad_id_duration, pad_id_rest, gpu=True).cuda()\n",
    "\n",
    "gen_opt = optim.Adam(gen.parameters(), lr=g_lr_pretrain)\n",
    "gen_adv_opt = optim.Adam(gen.parameters(), lr=g_lr_adv)\n",
    "mle_criterion = nn.NLLLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20daa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(opt, loss, model=None, retain_graph=False):\n",
    "    opt.zero_grad()\n",
    "    loss.backward(retain_graph=retain_graph)\n",
    "    if model is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f4c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen_epoch(model, data_loader, criterion, optimizer, cuda=True):\n",
    "    total_loss = 0\n",
    "    for data in data_loader:\n",
    "        lyr, mel, target = data['lyric'], data['melody'], data['target']\n",
    "        if cuda:\n",
    "            lyr, mel, target = lyr.cuda(), mel.cuda(), target.cuda()\n",
    "\n",
    "        hidden = model.init_hidden(data_loader.batch_size)\n",
    "        pred = model.forward(lyr, mel, hidden)\n",
    "        loss = criterion(pred, target.view(-1))\n",
    "        optimize(optimizer, loss, model)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_generator(gen, gen_opt, data_loader, criterion, epochs):\n",
    "    \"\"\"\n",
    "    Max Likelihood Pre-training for the generator\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    progress = tqdm(range(epochs))\n",
    "    for epoch in progress:\n",
    "        pre_loss = train_gen_epoch(gen, data_loader, criterion, gen_opt)\n",
    "        losses.append(pre_loss)\n",
    "        progress.set_description(\"EPOCH: {}, LOSS: {}\".format(epoch, pre_loss))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_lyrics_train = torch.LongTensor(train_lyrics)\n",
    "sequences_notes_train = torch.LongTensor(train_notes)\n",
    "sequences_durations_train = torch.LongTensor(train_durations)\n",
    "sequences_rests_train = torch.LongTensor(train_rests)\n",
    "\n",
    "sequences_notes_train = sequences_notes_train.unsqueeze(2)\n",
    "sequences_durations_train = sequences_durations_train.unsqueeze(2)\n",
    "sequences_rests_train = sequences_rests_train.unsqueeze(2)\n",
    "\n",
    "train_melodies = torch.cat([sequences_notes_train, sequences_durations_train, sequences_rests_train], dim=2)\n",
    "\n",
    "gen_data_iter = GenDataIter(sequences_lyrics_train, train_melodies, batch_size, max_seq_len, start_letter=start_id_lyr)\n",
    "\n",
    "train_melodies.shape, sequences_lyrics_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40563f1a",
   "metadata": {},
   "source": [
    "## Pretraining generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de1009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses_pretrain = pretrain_generator(gen, gen_opt, gen_data_iter.loader, mle_criterion, pretrain_epochs_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee5516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_pretrain)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.title(\"Pretraining losses generator\")\n",
    "plt.savefig(\"figures/relgan/pretraining_gen_full.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5086baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_start_and_end(lyrics):\n",
    "    clean_lyrics = []\n",
    "    for lyric in lyrics:\n",
    "        removal_list = [\"BOS\",\"EOS\", \"eos\", \"bos\"]\n",
    "        lyric_list = lyric.split()\n",
    "        final_list = [word for word in lyric_list if word not in removal_list]\n",
    "        final_string = ' '.join(final_list)\n",
    "        clean_lyrics.append(final_string)\n",
    "\n",
    "    return clean_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisDataIter:\n",
    "    def __init__(self, pos_samples, neg_samples):\n",
    "        self.batch_size = 32\n",
    "        self.max_seq_len = 20\n",
    "        self.start_letter = 0\n",
    "\n",
    "        self.loader = DataLoader(\n",
    "            dataset=GANDataset(self.__read_data__(pos_samples, neg_samples)),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True)\n",
    "\n",
    "    def __read_data__(self, pos_samples, neg_samples):\n",
    "        \"\"\"\n",
    "        input: same as target, but start with start_letter.\n",
    "        \"\"\"\n",
    "        inp, target = self.prepare(pos_samples, neg_samples)\n",
    "        all_data = [{'input': i, 'target': t} for (i, t) in zip(inp, target)]\n",
    "        return all_data\n",
    "\n",
    "    def random_batch(self):\n",
    "        idx = random.randint(0, len(self.loader) - 1)\n",
    "        return list(self.loader)[idx]\n",
    "\n",
    "    def prepare(self, pos_samples, neg_samples):\n",
    "        \"\"\"Build inp and target\"\"\"\n",
    "        inp = torch.cat((pos_samples, neg_samples), dim=0).long().detach()  # !!!need .detach()\n",
    "        target = torch.ones(inp.size(0)).long()\n",
    "        target[pos_samples.size(0):] = 0\n",
    "\n",
    "        # shuffle\n",
    "        perm = torch.randperm(inp.size(0))\n",
    "        inp = inp[perm].cuda()\n",
    "        target = target[perm].cuda()\n",
    "\n",
    "        return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = RelGAN_D(d_embed_dim, max_seq_len, num_rep, vocab_size, pad_id_lyr, gpu=True).cuda()\n",
    "dis_opt = optim.Adam(dis.parameters(), lr=d_lr_pre)\n",
    "dis_opt_adv = optim.Adam(dis.parameters(), lr=d_lr_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee88194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(d_out_real, d_out_fake, gen_samples=None, real_samples=None, loss_type='JS'):\n",
    "    \"\"\"Get different adversarial losses according to given loss_type\"\"\"\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if loss_type == 'standard':  # the non-satuating GAN loss\n",
    "        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n",
    "        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = bce_loss(d_out_fake, torch.ones_like(d_out_fake))\n",
    "\n",
    "    elif loss_type == 'JS':  # the vanilla GAN loss\n",
    "        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n",
    "        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = -d_loss_fake\n",
    "\n",
    "    elif loss_type == 'KL':  # the GAN loss implicitly minimizing KL-divergence\n",
    "        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n",
    "        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = torch.mean(-d_out_fake)\n",
    "\n",
    "    elif loss_type == 'hinge':  # the hinge loss\n",
    "        d_loss_real = torch.mean(nn.ReLU(1.0 - d_out_real))\n",
    "        d_loss_fake = torch.mean(nn.ReLU(1.0 + d_out_fake))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = -torch.mean(d_out_fake)\n",
    "\n",
    "    elif loss_type == 'tv':  # the total variation distance\n",
    "        d_loss = torch.mean(nn.Tanh(d_out_fake) - nn.Tanh(d_out_real))\n",
    "        g_loss = torch.mean(-nn.Tanh(d_out_fake))\n",
    "\n",
    "    elif loss_type == 'rsgan':  # relativistic standard GAN\n",
    "        d_loss = bce_loss(d_out_real - d_out_fake, torch.ones_like(d_out_real))\n",
    "        g_loss = bce_loss(d_out_fake - d_out_real, torch.ones_like(d_out_fake))\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Divergence '%s' is not implemented\" % loss_type)\n",
    "\n",
    "    if gen_samples == None or real_samples == None:\n",
    "        return g_loss, d_loss\n",
    "\n",
    "    r_s = torch.argmax(real_samples, dim=2)\n",
    "    r_s = tokenizer_lyr.sequences_to_texts(r_s.cpu().numpy())\n",
    "    g_s = torch.argmax(gen_samples, dim=2)\n",
    "    g_s = tokenizer_lyr.sequences_to_texts(g_s.cpu().numpy())\n",
    "\n",
    "    r_s = remove_start_and_end(r_s)\n",
    "    g_s = remove_start_and_end(g_s)\n",
    "\n",
    "    bleus_4, precisions, recalls = [], [], []\n",
    "    try:\n",
    "        for test_ref, test_pred in zip(r_s, g_s):\n",
    "            bleu4 = sentence_bleu(test_ref, test_pred, smoothing_function=chencherry.method1)\n",
    "            bleus_4.append(bleu4)\n",
    "            test_ref_set = set(test_ref.split())\n",
    "            test_pred_set = set(test_pred.split())\n",
    "            prec = precision(test_ref_set, test_pred_set)\n",
    "            rec = recall(test_ref_set, test_pred_set)\n",
    "            prec = 1e-10 if (prec == None) else prec\n",
    "            rec = 1e-10 if (rec == None) else rec\n",
    "\n",
    "            precisions.append(prec)\n",
    "            recalls.append(rec)\n",
    "\n",
    "        bleus_4 = torch.tensor(bleus_4)\n",
    "        bleu = torch.mean(bleus_4)\n",
    "        \n",
    "        precisions = torch.tensor(precisions)\n",
    "        precision_mean = torch.mean(precisions)\n",
    "        \n",
    "        recalls = torch.tensor(recalls)\n",
    "        recall_mean = torch.mean(recalls)\n",
    "\n",
    "        g_loss += -torch.log(bleu) - torch.log(precision_mean) - torch.log(recall_mean)\n",
    "\n",
    "    except KeyError:\n",
    "        g_loss += torch.tensor(20.0)\n",
    "        return g_loss, d_loss\n",
    "    \n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_discriminator(gen, dis, dis_opt, train_data, epochs, start_letter=1):\n",
    "    d_losses = []\n",
    "    progress = tqdm(range(epochs))\n",
    "    for step in progress:\n",
    "        rand_batch = train_data.random_batch()\n",
    "        real_samples = rand_batch['target']\n",
    "        mels = rand_batch['melody']\n",
    "        gen_samples = gen.sample(mels, batch_size, batch_size, one_hot=True, start_letter=start_letter)\n",
    "        real_samples, gen_samples = real_samples.cuda(), gen_samples.cuda()\n",
    "        real_samples = F.one_hot(real_samples, vocab_size).float()\n",
    "\n",
    "        # ===Train===\n",
    "        d_out_real = dis(real_samples)\n",
    "        d_out_fake = dis(gen_samples)\n",
    "        _, d_loss = get_losses(d_out_real, d_out_fake, 'JS')\n",
    "        optimize(dis_opt, d_loss, dis)\n",
    "        d_losses.append(d_loss.item())\n",
    "        progress.set_description('d_loss: %.4f' % (d_loss))\n",
    "\n",
    "    return d_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a52af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_losses_pretrain = pretrain_discriminator(gen, dis, dis_opt, gen_data_iter, 100, start_letter=start_id_lyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af491360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dis, \"RelGAN_Discriminator_melody_conditioned_full_dataset_pretrain.pth\")\n",
    "dis = torch.load(\"RelGAN_Discriminator_melody_conditioned_full_dataset_pretrain.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d_losses_pretrain)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.title(\"Pretraining losses discriminator\")\n",
    "plt.savefig(\"figures/relgan/pretraining_dis_full.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12293b",
   "metadata": {},
   "source": [
    "## Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_train_generator(gen, dis, gen_adv_opt, train_data, g_step, vocab_size,\n",
    "                        batch_size, loss_type=\"rsgan\", start_letter=1):\n",
    "    total_loss = 0\n",
    "    for step in range(g_step):\n",
    "        rand_batch = train_data.random_batch()\n",
    "        real_samples = rand_batch['target']\n",
    "        mels = rand_batch['melody']\n",
    "        gen_samples = gen.sample(mels, batch_size, batch_size, one_hot=True, start_letter=start_letter)\n",
    "        real_samples, gen_samples = real_samples.cuda(), gen_samples.cuda()\n",
    "        real_samples = F.one_hot(real_samples, vocab_size).float()\n",
    "\n",
    "        # ===Train===\n",
    "        d_out_real = dis(real_samples)\n",
    "        d_out_fake = dis(gen_samples)\n",
    "        g_loss, _ = get_losses(d_out_real, d_out_fake, real_samples, gen_samples, loss_type)\n",
    "\n",
    "        optimize(gen_adv_opt, g_loss, gen)\n",
    "        total_loss += g_loss.item()\n",
    "\n",
    "    return total_loss / g_step if g_step != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ab4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_train_discriminator(gen, dis, train_data, dis_opt, d_step, batch_size,\n",
    "                            vocab_size, loss_type=\"rsgan\", start_letter=1):\n",
    "    total_loss = 0\n",
    "    for step in range(d_step):\n",
    "        rand_batch = train_data.random_batch()\n",
    "        real_samples = rand_batch['target']\n",
    "        mels = rand_batch['melody']\n",
    "        gen_samples = gen.sample(mels, batch_size, batch_size, one_hot=True, start_letter=start_letter)\n",
    "        real_samples, gen_samples = real_samples.cuda(), gen_samples.cuda()\n",
    "        real_samples = F.one_hot(real_samples, vocab_size).float()\n",
    "\n",
    "        # ===Train===\n",
    "        d_out_real = dis(real_samples)\n",
    "        d_out_fake = dis(gen_samples)\n",
    "        _, d_loss = get_losses(d_out_real, d_out_fake, loss_type)\n",
    "\n",
    "        optimize(dis_opt, d_loss, dis)\n",
    "        total_loss += d_loss.item()\n",
    "\n",
    "    return total_loss / d_step if d_step != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac26f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixed_temperature(temper, i, N, adapt):\n",
    "    \"\"\"A function to set up different temperature control policies\"\"\"\n",
    "\n",
    "    if adapt == 'no':\n",
    "        temper_var_np = 1.0  # no increase, origin: temper\n",
    "    elif adapt == 'lin':\n",
    "        temper_var_np = 1 + i / (N - 1) * (temper - 1)  # linear increase\n",
    "    elif adapt == 'exp':\n",
    "        temper_var_np = temper ** (i / N)  # exponential increase\n",
    "    elif adapt == 'log':\n",
    "        temper_var_np = 1 + (temper - 1) / np.log(N) * np.log(i + 1)  # logarithm increase\n",
    "    elif adapt == 'sigmoid':\n",
    "        temper_var_np = (temper - 1) * 1 / (1 + np.exp((N / 2 - i) * 20 / N)) + 1  # sigmoid increase\n",
    "    elif adapt == 'quad':\n",
    "        temper_var_np = (temper - 1) / (N - 1) ** 2 * i ** 2 + 1\n",
    "    elif adapt == 'sqrt':\n",
    "        temper_var_np = (temper - 1) / np.sqrt(N - 1) * np.sqrt(i) + 1\n",
    "    else:\n",
    "        raise Exception(\"Unknown adapt type!\")\n",
    "\n",
    "    return temper_var_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec04359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_temperature(i, N, temp_adpt=\"exp\"):\n",
    "    return get_fixed_temperature(1000.0, i, N, temp_adpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e91a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_train(gen, dis, gen_adv_opt, dis_adv_opt, train_data, vocab_size,\n",
    "                      batch_size, epochs, start_letter=1):\n",
    "    progress = tqdm(range(epochs))\n",
    "    g_losses, d_losses = [], []\n",
    "    for adv_epoch in progress:\n",
    "        g_loss = adv_train_generator(gen, dis, gen_adv_opt, train_data, 5,\n",
    "                                     vocab_size, batch_size, loss_type = 'rsgan', start_letter=start_letter)  # Generator\n",
    "        d_loss = adv_train_discriminator(gen, dis, train_data, dis_adv_opt, 1,\n",
    "                                         batch_size, vocab_size, loss_type = 'rsgan', start_letter=start_letter)  # Discriminator\n",
    "        gen.temperature = update_temperature(adv_epoch, epochs)  # update temperature\n",
    "        g_losses.append(g_loss)\n",
    "        d_losses.append(d_loss)\n",
    "        progress.set_description('g_loss: %.4f, d_loss: %.4f, temperature: %.4f' % (g_loss, d_loss, gen.temperature))\n",
    "    return g_losses, d_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3e911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adv_g_losses, adv_d_losses = adversarial_train(gen, dis, gen_adv_opt, dis_opt_adv, gen_data_iter,\n",
    "                                               vocab_size, batch_size, 10000,\n",
    "                                               start_letter=start_id_lyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6522de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_pretrain)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.title(\"Pretraining losses generator\")\n",
    "plt.savefig(\"figures/relgan/pretraining_full.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ad7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adv_g_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.title(\"Adversarial losses generator\")\n",
    "plt.savefig(\"figures/relgan/adv_gen_full_mod_loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe190b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adv_d_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.title(\"Adversarial losses discriminator\")\n",
    "plt.savefig(\"figures/relgan/adv_dis_full_mod_loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ff5646",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cddf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_is = np.random.choice(test_inds, 80)\n",
    "test_nos = np.expand_dims(np.array([notes[i] for i in test_is]), axis=2)\n",
    "test_dus = np.expand_dims(np.array([durations[i] for i in test_is]), axis=2)\n",
    "test_res = np.expand_dims(np.array([rests[i] for i in test_is]), axis=2)\n",
    "test_lyr = [lyrics[i] for i in test_is]\n",
    "\n",
    "test_mels = np.concatenate([test_nos, test_dus, test_res], axis=2)\n",
    "test_mels = torch.LongTensor(test_mels)\n",
    "test_mels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cdb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gen.sample(test_mels, 80, 32, start_letter=start_id_lyr)\n",
    "preds = tokenizer_lyr.sequences_to_texts(samples.cpu().detach().numpy())\n",
    "orig = tokenizer_lyr.sequences_to_texts(test_lyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efcafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = remove_start_and_end(preds)\n",
    "orig = remove_start_and_end(orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d15221",
   "metadata": {},
   "source": [
    "## Rouge Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8196c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rouge = ROUGEScore()\n",
    "\n",
    "r_f_measure_1, r_precision_1, r_recall_1 = [], [], []\n",
    "r_f_measure_2, r_precision_2, r_recall_2 = [], [], []\n",
    "r_f_measure_l, r_precision_l, r_recall_l = [], [], []\n",
    "for test_ref, test_pred in tqdm(zip(orig, preds)):\n",
    "    rouge_dict = rouge(test_pred, test_ref)\n",
    "    rouge1_fmeasure = rouge_dict[\"rouge1_fmeasure\"]\n",
    "    rouge1_precision = rouge_dict[\"rouge1_precision\"]\n",
    "    rouge1_recall = rouge_dict[\"rouge1_recall\"]\n",
    "    rouge2_fmeasure = rouge_dict[\"rouge2_fmeasure\"]\n",
    "    rouge2_precision = rouge_dict[\"rouge2_precision\"]\n",
    "    rouge2_recall = rouge_dict[\"rouge2_recall\"]\n",
    "    rougeL_fmeasure = rouge_dict[\"rougeL_fmeasure\"]\n",
    "    rougeL_precision = rouge_dict[\"rougeL_precision\"]\n",
    "    rougeL_recall = rouge_dict[\"rougeL_recall\"]\n",
    "    \n",
    "    r_f_measure_1.append(rouge1_fmeasure)\n",
    "    r_precision_1.append(rouge1_precision)\n",
    "    r_recall_1.append(rouge1_recall)\n",
    "    r_f_measure_2.append(rouge2_fmeasure)\n",
    "    r_precision_2.append(rouge2_precision)\n",
    "    r_recall_2.append(rouge2_recall)\n",
    "    r_f_measure_l.append(rougeL_fmeasure)\n",
    "    r_precision_l.append(rougeL_precision)\n",
    "    r_recall_l.append(rougeL_recall)\n",
    "    \n",
    "print(np.mean(r_f_measure_1), np.mean(r_precision_1), np.mean(r_recall_1))\n",
    "print(np.mean(r_f_measure_2), np.mean(r_precision_2), np.mean(r_recall_2))\n",
    "print(np.mean(r_f_measure_l), np.mean(r_precision_l), np.mean(r_recall_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9e398",
   "metadata": {},
   "source": [
    "## BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0915d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleus_4, bleus_3, bleus_2 = [], [], []\n",
    "for test_ref, test_pred in tqdm(zip(orig, preds)):\n",
    "    bleu4 = sentence_bleu(test_ref, test_pred, smoothing_function=chencherry.method7)\n",
    "    bleu3 = sentence_bleu(test_ref, test_pred, weights=[1/3, 1/3, 1/3], smoothing_function=chencherry.method7)\n",
    "    bleu2 = sentence_bleu(test_ref, test_pred, weights=[1/2, 1/2], smoothing_function=chencherry.method7)\n",
    "    bleus_4.append(bleu4)\n",
    "    bleus_3.append(bleu3)\n",
    "    bleus_2.append(bleu2)\n",
    "    \n",
    "np.mean(bleus_2), np.mean(bleus_3), np.mean(bleus_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76be134",
   "metadata": {},
   "source": [
    "## BERT Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = BERTScore()\n",
    "scores = bertscore(orig, preds)\n",
    "np.mean(scores[\"precision\"]), np.mean(scores[\"recall\"]), np.mean(scores[\"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36351999",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1eedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_ratios, neu_ratios, neg_ratios = [], [], []\n",
    "for o, p in zip(orig, preds):\n",
    "    ss_orig = sid.polarity_scores(o)\n",
    "    ss_pred = sid.polarity_scores(p)\n",
    "    \n",
    "    ori_neg = ss_orig[\"neg\"]\n",
    "    ori_neu = ss_orig[\"neu\"]\n",
    "    ori_pos = ss_orig[\"pos\"]\n",
    "    \n",
    "    pred_neg = ss_pred[\"neg\"]\n",
    "    pred_neu = ss_pred[\"neu\"]\n",
    "    pred_pos = ss_pred[\"pos\"]\n",
    "    \n",
    "    if ori_neg > pred_neg:\n",
    "        neg_ratios.append(pred_neg/ori_neg)\n",
    "    elif ori_neg == 0 and pred_neg == 0:\n",
    "        neg_ratios.append(1)\n",
    "    else:\n",
    "        neg_ratios.append(ori_neg/pred_neg)\n",
    "        \n",
    "\n",
    "    if ori_neu > pred_neu:\n",
    "        neu_ratios.append(pred_neu/ori_neu)\n",
    "    elif ori_neu == 0 and pred_neu == 0:\n",
    "        neu_ratios.append(1)\n",
    "    else:\n",
    "        neu_ratios.append(ori_neu/pred_neu)\n",
    "\n",
    "    if ori_pos > pred_pos:\n",
    "        pos_ratios.append(pred_pos/ori_pos)\n",
    "    elif ori_pos == 0 and ori_pos == 0:\n",
    "        pos_ratios.append(1)\n",
    "    else:\n",
    "        pos_ratios.append(ori_pos/pred_pos)\n",
    "\n",
    "np.mean(pos_ratios), np.mean(neu_ratios), np.mean(neg_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be711f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
